# ENDB2 SESSION 03

This contain solution  of END Batch2  SESSION 03

# TASK /OBJECTIVE 


<a href="url"><img src="https://github.com/jitendramishra1024/ENDB2/blob/main/SESSION-03/images/TASK.PNG" align="center" height="457" width="764" ></a>



# EXPLANATION 

## ARCHITECTURE 

IMAGE

## JUPYTER NOTEBOOK 


## Data Representation:

INPUT1: ONE MNIST IMAGE 
INPUT2: ONE RANDOM IMAGE FROM 0 to9 

OUTPUT 1: IMAGE PREDICTION 
OUTPUT 2: RANDOM IMAGE LABEL+RANDOM NUMBER 

IMAGE 

## Data Generation Strategy:

IMAGE

## combined the two inputs

IMAGE Reduced to 1X1X10 
DIGIT Reduced to 1X1X10

then concatenated 

IMAGE 

## WHICH LOSS FUNCTION IS TAKEN AND WHY 

cross entropy loss :

cross entropy loss, which is basically log_softmax + nll_loss (negative log likelihood).

It is generally  used for multiclass classifcation .

## RESULT EVALUATION 

HOW RESULT IS EVALUATED 

10000 images and 10000 numbers passed through test to evaluate accuracy at every epoch 

IMAGE 

## TRAIN AND TEST LOGS 

IMAGE 

## TRAIN TEST ACCURACY AT 30 TH EPOCH 

IMAGE

## FINAL RESULT EVALUATION FOR 1BATCH 

IMAGE









