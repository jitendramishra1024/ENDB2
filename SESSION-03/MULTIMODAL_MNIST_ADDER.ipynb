{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MULTIMODAL_MNIST_ADDER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtpSEK_yhvr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "882909f0-cea1-42d3-ca9f-276673d4bcc6"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt #for plotting\n",
        "# using cuda enabled GPU  if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFXmuuH2KK_j"
      },
      "source": [
        "# downloading/loading MNIST train dataset\n",
        "mnist_train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz0NQXQSKhCu",
        "outputId": "eddb3afb-51d1-4505-cecc-cb317ad1604e"
      },
      "source": [
        "print(\"Total number of Images in Dataset\",len(mnist_train_data))\n",
        "# NOTE :Basically this contains 60000 tuples of ((1,28,28),1) =(image,label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Images in Dataset 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "h9EmNk5TK4Hp",
        "outputId": "5acabfab-9ba2-4c7b-c19c-8f10b8e09418"
      },
      "source": [
        "# plotting sample of dataset\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "#randomly selecting index from dataset\n",
        "rand_idx = torch.randint(len(mnist_train_data), size=(cols*rows,))\n",
        "for i, idx in enumerate(rand_idx):\n",
        "  #unpacking data set for img and label\n",
        "  img, label = mnist_train_data[idx.item()]\n",
        "  figure.add_subplot(rows, cols, i+1)\n",
        "  plt.title(label)\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAckAAAHRCAYAAAABukKHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRV1fXA8f2SkIAJBjQMIjLKoFAZpIiCIA6UigUtsAArg4pLKYoySGsFmRzKQiWK4ASmQB1RLFooxcU8iIBMYgqVUYREaEgCwQBC7u+Pnz/7O/fczXt5vOS+vHw/a9212Hvtd9+xec3mcM47NyAijgAAAEuc3wMAACBa0SQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQ91K1bVxYuXCjHjh2TrKwsmTZtmsTHx/s9LMSwoUOHysaNG+XUqVOSkZHh93BQTsydO1cOHz4s+fn5smvXLrn//vv9HlJUcrjMa+HChU5GRoaTlJTk1KhRw9m+fbvzyCOP+D4urti97rrrLqdHjx7OjBkznIyMDN/Hw1U+rquvvtpJTEx0RMRp0qSJk5WV5bRu3dr3cUXTxUzSQ/369eWDDz6Q06dPy/fffy+LFy+WZs2a+T0sxLCPP/5YFixYIDk5OX4PBeVIZmamnDlzRkREHMcRx3GkYcOGPo8qutAkPaSnp0vfvn2lUqVKUqtWLfn1r38tixcv9ntYABBx06dPl5MnT8quXbskKytLFi1a5PeQogpN0sOqVaukWbNmcvz4cTl06JBs2rRJ/va3v/k9LACIuKFDh0rlypWlQ4cOMn/+fDl9+rTfQ4oqNEmXQCAgixcvlvnz50tycrJceumlUrVqVZk8ebLfQwOAElFUVCRr166V2rVry5AhQ/weTlShSbpccsklUrduXXnllVfkzJkzcuzYMcnIyJDbb7/d76EBQIlKSEhgTdKFJumSk5Mje/fulSFDhkh8fLykpqbKwIEDZfv27X4PDTEsPj5ekpKSJD4+3vgzUFKqVasmffr0keTkZImLi5MuXbpIv379ZOnSpX4PLer4vsU22q4WLVo4y5cvd44dO+YcPXrUef/9953q1av7Pi6u2L3GjRvnuI0bN873cXHF7pWWluasWLHCyc3NdfLz853t27c7gwcP9n1c0XYFfvoDAABw4Z9bAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFDQJAEAUNAkAQBQ0CQBAFAk+D0AFE/dunWNeMeOHVbNRx99ZMSDBg0qySHhAlSrVs3KffbZZ1auRYsWRvzJJ59YNT169IjcwFAmNW7c2MoNHTrUiKtUqWLVBAIBK+euu+OOO4K+/9q1a63cl19+acRjx461ak6cOBH03n5hJgkAgIImCQCAgiYJAICCJgkAgIKNOxcoMTHRyp07d+688YVo2bKlEScnJ1s1bdu2NeIKFSpYNT/++GPExoTQXXTRRUa8aNEiq+aaa66xco7jlNiYEJ2qV69uxL/73e+smtGjRxtxSkqKVeP+zHnx2rjj/syF8hls3769lbvhhhuMuGnTplZN7969rVy0bOZhJgkAgIImCQCAgiYJAICCNcliatasmRG/8cYbVk3FihWNuFu3blZNdnZ20PeqVauWlZs2bVrQ11WqVMmI4+L4u1C0GDlypBFfe+21Vs2mTZusXMOGDUtsTIhOV111lRHXrFnTqtm7d68R169f36o5cuSIEX/44YdWjdea5ObNm404MzPTqnG/35gxY6ya1q1bG/Ftt91m1bRr187KeR2q4Qd+ewIAoKBJAgCgoEkCAKCgSQIAoGDjznm4T88XEZkyZYoRuzfpeKlXr56Vc2/c8ToUYN68eVaudu3aQd9v8uTJRnz69Omgr0Hkef3cH3vssaCv8/q5u780jti3cuXK88bRYPv27UZ8/fXXWzXujTtlDTNJAAAUNEkAABQ0SQAAFDRJAAAUbNz5yZAhQ6zc888/b+WKioqM+PXXX7dq1q1bZ8QbNmywapo3b27ETz/9tFXjtQju5nUqxZtvvhn0dSh5XqcjVa1a1Yi3bt1q1axevdrKuTdjAdHAfSqY19M83Kf55OfnWzU7duyI7MAiiJkkAAAKmiQAAAqaJAAAinK7JpmQYP6np6enWzVLliyxcgMGDDDi3NzcoO/l9aT59evXG3EohxKIiEydOtWIvU7dP3fuXEj3QmR16tTJiLt27Rr0NePGjbNyl112WcTGBERKixYtrNy7775rxF5PIXEcx4hfeuklqyYrK+sCR1dymEkCAKCgSQIAoKBJAgCgoEkCAKAotxt3Jk2aZMQVKlSwaj744AMrF8pGHffGizVr1lg1oWzU8dpM9Pjjjxux+3AD+OfQoUNG7HWIxLZt24x40aJFVs0LL7wQ2YEBYejXr58Rz5w506px/x7zeuKQ+6k2Ze1gDGaSAAAoaJIAAChokgAAKMrtmqT738m9nvq9c+dOK5eSkmLEXmuZmzZtOu9rvHitP44YMSLo6xA9du/ebcTt27cvsfeK5gOhEd0aNGhg5dx7HUREBg0aZMSJiYlB792/f38r9+GHH4Y+uCjETBIAAAVNEgAABU0SAAAFTRIAAEW53bjzyCOPGHFeXp5Vc99991m5m266yYjT0tKCvte+ffus3IwZM4yYL5CjONybwwBNamqqEa9evdqqidSTZ7p3727l3JvMvDZERjNmkgAAKGiSAAAoaJIAACgCIuIErYpBhYWFRlxQUGDVeH159uKLLzZi91O3vUyYMCGkHCAicvToUStXpUoVI+7SpYtVs3z58hIbE8qGDh06WLk//elPRvyrX/3KqgkEAlYulN9todznxIkTRnz99ddbNZmZmcV+r9LCTBIAAAVNEgAABU0SAAAFTRIAAEW5OEygb9++Vs69KefSSy8N6V6hLGa7T9T3esIHIOL9FBmvDWMbNmwwYjbpQESkXbt2Rrx06VKrJiHB/DWfm5tr1bz11ltWLpyNO15PE3E/BemJJ56waryeHhItmEkCAKCgSQIAoKBJAgCgoEkCAKAoFxt36tWrZ+W8ToYIxcmTJ4149OjRVs0bb7xhxOfOnQvrvRD7fv/731u5ypUrWzn3xh1AROSbb74x4oyMDKumYsWKRvz8889bNe4ndYSrZ8+eVq5+/fpGHM6GID8xkwQAQEGTBABAQZMEAEARk2uSw4cPN2L3Kfih2rdvn5UbOnSoES9evDisewMiIrfccovfQ0AZlpOTY8QPPfRQqb5/7969jbhhw4ZWjXsNMjs7u0THFGnMJAEAUNAkAQBQ0CQBAFDQJAEAUJS5jTvuE+3dX9wXERk0aFCx73vq1Ckr1717dyv39ddfF/vewIXatWuX30NAOdevXz8r5356iNdBAe6DCqZNmxbZgZUwZpIAAChokgAAKGiSAAAoonpNsmrVqlZu4cKFRux+Mne4hg0bZuVYf0S0WL9+vd9DQIzw+r1at25dK+c+rHzEiBFWTWJiohHv3LnTqunWrZsRHzx4MKRxRgtmkgAAKGiSAAAoaJIAAChokgAAKKJm405cnN2vn3nmGSsXqY06y5cvN+L58+dH5L7A+SQnJxtx06ZNfRoJYkH//v2NuFWrVlZNpUqVjNi9IUdEJC0tzcq5Dwbw2sjo/r35+uuvWzVZWVlWrixhJgkAgIImCQCAgiYJAIAiatYk3Ws1IiX7lO158+YZ8bFjx0rsvYD/414fuvLKK0N6nXvt0mt9qKioyIibNGli1WRmZob0fig5tWrVsnKNGze2cu79F88++6xVEwgEjNjrgPFQeP3+cz88YsqUKVZNbm5uWO9XljCTBABAQZMEAEBBkwQAQEGTBABAETUbd5o1a1Zi9/7oo4+s3Lvvvlti7weIeB8UcNVVV4V1r3feeSdozZ49e4zY68kOHTt2NGKeLnJh3L+3UlJSrBr3l/cfeOABq8brde4DVkLZlONVs23bNiOeNGmSVbNixQorVx425YSCmSQAAAqaJAAACpokAAAKmiQAAIqo2bjjdeJEuD7//HMj9looz8/Pj9j7AV5WrVpl5byethCK7777zoiTkpKsGvfpK4sWLbJqtm/fHtb7Q6RevXpWbt26dUbstQHHzf1zEgltU47XRpoDBw4YsdepOAsWLDDiwsLCoO+F/2ImCQCAgiYJAICCJgkAgCJq1iTXrFlj5WbPnm3lBg4caMSvvPKKVfPUU08ZcV5e3gWODii+w4cPWzn3mpXXoRYbN260cn/961/Pex8Rkezs7OIOEcXg9b95QkLxf4UeOnTIyu3bt8/KTZs2zYiXLVtm1eTk5BT7/VE8zCQBAFDQJAEAUNAkAQBQ0CQBAFAERCT4t1gBACiHmEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSQIAoKBJAgCgoEkCAKCgSbqcOHHCuM6ePSsvv/yy38NCObB8+XIpLCz8+bO3c+dOv4eEGJaYmCgzZ86U/fv3y/Hjx2XLli3StWtXv4cVdWiSLpUrV/75qlmzphQWFsq8efP8HhbKiYcffvjnz1/Tpk39Hg5iWEJCghw8eFA6deokqampMmbMGPnggw+kbt26fg8tqiT4PYBo1rNnTzly5IisXr3a76EAQET98MMPMmHChJ/jhQsXyr59++Taa6+VAwcO+Diy6MJM8jwGDhwoc+bM8XsYKEeee+45OXr0qKxZs0Y6derk93BQjlSvXl0aN24sX3/9td9DiToOl33VqVPHOXv2rFOvXj3fx8JVPq62bds6KSkpTmJiojNgwADn+PHjToMGDXwfF1fsXwkJCc5nn33mvPbaa76PJdqueBEZL7A8/PDDUqFCBUlPT/d7KCgnDh06JGfOnJFz587Jtm3bpGPHjiIismHDBp9HhlgWCATk7bffloSEBBkwYIAUFRX5PaSowj+3KgYMGCCzZ8/2exgoxxzHkUAg4PcwEONmzZolNWrUkJ49e8rZs2f9Hk5U8n06G23X9ddf7xQUFDgpKSm+j4WrfFypqalOly5dnKSkJCc+Pt65++67nYKCAqdRo0a+j40rdq9XX33V+fzzz53k5GTfxxLFl+8DiLrrtddec+bMmeP7OLjKz5WWluZs2LDBOX78uJObm+t8/vnnzq233ur7uLhi96pTp47jOI5TWFjonDhx4ufr7rvv9n1s0XQFfvoDAABwYU0SAAAFTRIAAAVNEgAABU0SAAAFTRIAAAVNEgAABU0SAAAFTRIAAAVNEgAABU0SAAAFTRIAAAVNEgAABU0SAAAFTRIAAAVNEgAABU0SAAAFTRIAAAVNEgAARYLfAwAARJ9+/foZcZMmTayasWPHGnFcnD3v2rJlixF369bNqsnKygpniKWCmSQAAAqaJAAACpokAACKgIg4fg8CQOm45JJLjHjChAkhve7UqVNGPH36dKtm//79YY8Lpadx48ZWrn///lZu5MiRRpyYmBj03oFAwMo5jtliNm7caNX07t3byn333XdB3680MJMEAEBBkwQAQEGTBABAQZMEAEDBxh0gRvXq1cvKPf3000bcqFEjq8a90cJLfn6+lXv77beNeNiwYUHvg8hKSkqycq1atTLid955x6qpU6dO0Hv/+9//tnI5OTlGfPnll4d17759+1q5Dz/8MOjrSgMzSQAAFDRJAAAUNEkAABSsSUaJm266ycotX77cyrm//D1+/PgSGhH84v7Cv4i9vue1btimTRsjvuWWW6wa9xfCQ/nyd7gSEnh+Qmm74oorrNy+ffuM2OtnfujQISs3depUI543b55V4/7C//Dhw62aKVOmeA/2/2FNEgCAMogmCQCAgiYJAICCJgkAgIKVdZ+4N+V4bdzxMm7cOCNesWKFVeOVQ3Rwb5zxevrBK6+8YuUuvvhiI47U5hovEydOtHKZmZlG3KVLF6umYcOGJTYmhCYvL8/KjRo1KujrZs6caeUKCgoiMqayjpkkAAAKmiQAAAqaJAAACpokAACKcrtxx71xZuXKlVZNpE6z8dqUE+pGnXDuzcad6PXQQw8Z8YsvvujTSP6X+6kgIiIvvPCClTt58qQRR8tpKDCdOHHCyqWnp/swkv/yOuGnLGEmCQCAgiYJAICCJgkAgCIm1yTD+aJ+qGuE7nVKr3XLTp06GbHXeqd73TDcNUrWH6PXY489ZuWeeeYZIw51vSYuzvz7bFFRkVWzaNGioO+/Z8+ekN4PCMfYsWOtXEkefFEamEkCAKCgSQIAoKBJAgCgoEkCAKAIiEiZXlX1e1HYa+NMpA4K8DJhwgQjjtSBB7hwDz74oBE/++yzVk1qamrQ+yxZssTKLVy40Ih37Nhh1Rw4cMCI9+/fH/S9gFAlJSVZOffvn9GjR1s17t/RXp/dW265xcrl5OQUc4Qlg5kkAAAKmiQAAAqaJAAAijJ3mID7oAC/udcIRUQ6d+5sxF7rpu61zFAPWC/J9U7oEhMTjdh9ULmIyNSpU43Y6+fu/jk/+eSTVs2mTZus3NmzZ0MaJ1BSGjVqZOUef/zxYt9nxowZVi5a1h+9MJMEAEBBkwQAQEGTBABAQZMEAEARNRt3vDaklOQmHfeGG69DAcJ9wkYom2vcGzhCPRSAp37444YbbjDiF198Mehrtm3bZuXcX7b+8ssvL2xgQCnxesJHKLZu3WrEn376aSSGU2qYSQIAoKBJAgCgoEkCAKCImjXJ0lx/FIncweCRWkv1ug/rj/6oVKmSlfP60n8wI0aMsHKsQaKs+OMf/2jEvXr1CvqauDh73jVr1iwjzsrKurCBlTJmkgAAKGiSAAAoaJIAAChokgAAKKJm40643E/cECndDS/jxo2LyOvYpBM9unXrZuVuvvnmoK9zb1p49NFHrZpFixYZccWKFa2a9957z8p98sknRrxu3Tqr5uDBg0HHiLLrjjvusHJem8yC6d69u5Vzf75ERAYOHGjEXk+1cXv99det3BtvvFGM0UUfZpIAAChokgAAKGiSAAAoaJIAACgCIhJ8NbYUhHpyjXuDi9fGnZLkHpPXuN0n/HTq1MmqCfcpICh5TZs2tXKLFy824tq1a1s1gUDAiEPZ6BAq972PHDli1bz55ptG/NRTT0Xs/RE5SUlJVs7r///u3xstW7a0ahITE4v9/u7PkkjkPqsNGza0cgcOHIjIvf3CTBIAAAVNEgAABU0SAABF1KxJRqNQ1klDecKI19qq+3UcJhDd3OuUEydOtGo6duxoxF7rPIWFhUa8ZMkSq6ZHjx5Wrnr16kHv7c4999xzVg3rlKWvRo0aRvzwww9bNU888YSVc68dHjp0yKo5d+6cESck2OfDXHbZZee9r0jk1iTT0tKsXF5eXkTu7RdmkgAAKGiSAAAoaJIAAChokgAAKNi48xOvTTpeT/jwqgvG7yeVoHguv/xyK+e1aaI0LVu2zIhbtGhh1aSmphrxtm3brBr35qKTJ09GYHQ4n7Vr1xrxddddF9LrVq1aZcReT+8oKCgw4iuuuMKq2bdvnxGX5Mad9PR0Kzdq1KiI3NsvzCQBAFDQJAEAUNAkAQBQ2N88Lae81hrDWX8U4aCAss5r7efVV1/1YST/dfPNNxuxe41SxF5v9Fq3rFu3rhFnZmZGYHQ4H/caoNeaoPuBByL2z9xLv379jHj06NFB3z8uzp4bFRUVBR2T14Ma3EaOHGnlpk6dasR+r+8XFzNJAAAUNEkAABQ0SQAAFDRJAAAU5XbjjvtJHV4HB4TCa1OO11PGER06dOhgxM2bN7dqtmzZUlrD8XTllVdauZo1axpxo0aNrBr3Bo1vvvnGqsnOzr7A0eF87rjjDivXsmVLI/bauOK1Wax3795GPHbsWKvG/VlJTEy0atwHBezcudOq6dmzp5X79ttvjdj9JBwRkfXr1xux1wagso6ZJAAACpokAAAKmiQAAIpysSYZ6uHloXCvQboPDkB0c39B2utg5wcffNDKuQ+O/uc//2nVHD9+/LyvERFp166dEXfp0sWq6dOnj5VLTk42Yq9xu3NeX1A/duyYlUPkVKpUycq51wnT0tKsGq/Pk/uzEsoh5KdPn7ZymzdvNuJ77rnHqjlw4EDQe3/11VdWrn379kbsdVDC0aNHg947mjGTBABAQZMEAEBBkwQAQEGTBABAERCRyDySOoq4N+osX748Yvf2WphG2XHu3DkjDveJ7Hl5eUHv7fXF7sqVK4f1fu7PXW5urlWzdOlSIx4wYIBV47WxA5HjPgBAROTdd98N617un7nXZ3XDhg1GPGnSJKvmH//4R1jvj//FTBIAAAVNEgAABU0SAAAFTRIAAEVMbtwJZTNG586djdhrc4/XEz7cr0PZsmzZMiNu0aKFVZOamhrWvUPZaBGKw4cPW7m33nrLiKdPn27VlPWTTWJB48aNrdztt99uxP3797dqvD6Hq1atMuJPPvnEqnn//feNOCsrK6RxInTMJAEAUNAkAQBQ0CQBAFCU+TXJ8ePHW7lQnvDhfnpHp06drBrWH2NfvXr1rNxFF10U9HXNmze3cjfeeGOx33/JkiVWbuPGjVYuOzu72PcGcOGYSQIAoKBJAgCgoEkCAKCgSQIAoCjzG3fC/cK2m9cmHa/DBAAA5QczSQAAFDRJAAAUNEkAABQJfg/gQnmtG65cudKIvQ4XcL+O9UcAgBszSQAAFDRJAAAUNEkAABQ0SQAAFGX+MAEAAEoKM0kAABQ0SQAAFDRJAAAUNEkAABQ0SQAAFDRJAAAUNEkAABQ0SQAAFDRJAAAUNEkAABQ0SQAAFDRJAAAUNEkAABQ0SQAAFDRJAAAUNEkAABQ0SQAAFDRJAAAUNEkAABQ0SQ9z586Vw4cPS35+vuzatUvuv/9+v4eEcqJPnz6SmZkpBQUFsnv3bunQoYPfQ0IMGzp0qGzcuFFOnTolGRkZfg8najlc5nX11Vc7iYmJjog4TZo0cbKyspzWrVv7Pi6u2L5uvfVWZ//+/c51113nBAIBp1atWk6tWrV8HxdX7F533XWX06NHD2fGjBlORkaG7+OJxitBYMnMzPz5z47jiOM40rBhQ9m8ebOPo0KsmzBhgkycOFG++OILERE5fPiwzyNCrPv4449FRKRNmzZSu3Ztn0cTnfjnVsX06dPl5MmTsmvXLsnKypJFixb5PSTEsLi4OGnTpo1Uq1ZNvvnmGzl48KBMmzZNKlas6PfQgHKNJqkYOnSoVK5cWTp06CDz58+X06dP+z0kxLAaNWpIYmKi9OrVS2688UZp2bKltGrVSsaMGeP30IByjSZ5HkVFRbJ27VqpXbu2DBkyxO/hIIYVFhaKiMi0adMkOztbcnJy5MUXX5Tbb7/d55EB5RtNMgQJCQnSsGFDv4eBGJaXlycHDx4Ux3F+zv3/PwPwB03SpVq1atKnTx9JTk6WuLg46dKli/Tr10+WLl3q99AQ4zIyMuSRRx6RatWqSZUqVWT48OHy97//3e9hIYbFx8dLUlKSxMfHG3+GyfctttF0paWlOStWrHByc3Od/Px8Z/v27c7gwYN9HxdX7F8JCQnO9OnTndzcXCcrK8t56aWXnKSkJN/HxRW717hx4xy3cePG+T6uaLoCP/0BAAC48M+tAAAoaJIAAChokgAAKGiSAAAoaJIAAChokgAAKGiSAAAoaJIAAChokgAAKGiSAAAoaJIAAChokgAAKGiSAAAoaJIAACgS/B4AAKD0JCTYv/b//Oc/W7lhw4YZcevWra2aHTt2RG5gUYqZJAAACpokAAAKmiQAAAqaJAAACjbuAFEmLs7+u2vjxo2NeOnSpVbNxo0bjfi3v/2tVVNUVHSBo0NZk5iYaMQzZ860au655x4r5ziOEaekpER2YGUEM0kAABQ0SQAAFDRJAAAUARFxglaVcTVr1rRyXbt2Dfq60aNHW7kmTZoY8ahRo6ya3NzcoPdevHixEWdnZwd9DWJPfHy8lZs4caKVe+KJJ4p978cee8zKvfzyy8W+D8oO9/qjiP158vq99sUXX1i5pk2bGnHVqlUvcHRlEzNJAAAUNEkAABQ0SQAAFDRJAAAUZX7jTvPmza2ce3NCamqqVXPNNdcYsdcXuCP1xWuve2/dutWI58yZY9VMnz7dyp09ezYiY0J06Ny5s5XzOiggFD/++KMRP/DAA1aN1+cMseM3v/mNlVuwYIERr1+/3qr5y1/+YuUmT55sxGzcAQAABpokAAAKmiQAAIoyd8D5+PHjjbh///5WTZ06dUppNOFzr4k+//zzVo37gGERvgxe1lWoUMGIn3zyyYjd+7333jNi1h/Ln82bN1u52bNnG3F6erpV43XwhNe9yiNmkgAAKGiSAAAoaJIAAChokgAAKKL6MIEpU6ZYuREjRhjxDz/8YNV4nWgfzNq1a63cW2+9Vez7iIjcd999RtyrVy+rxv2keS+nTp2ycoMGDTLijz76qHiDQ6kJ94kMoSgsLLRybdu2NeKvv/46rHuj/MnMzLRy7t+j9957b2kNJ6owkwQAQEGTBABAQZMEAEBBkwQAQBE1J+40bdrUynXv3j3o65599lkr99xzz0VkTOEaN26cEc+dO9eq+fjjj43Y67+/YsWKVi45OfkCR4fS4rU5K9yNOm5eJ6SwUQfhuuyyy/weQtRiJgkAgIImCQCAgiYJAIAiatYkvb5wf+WVV1q5vXv3GvH8+fNLbEyRsnv3bivnfoL4nj17QrpXIBCIyJhQtixbtsyIy8LnHtGpTZs2Vi4lJcXK7dq1qzSGE/WYSQIAoKBJAgCgoEkCAKCgSQIAoIiajTsHDx60cidOnLBy7i/hx8riclFRUUh1jhO1D22BS+/evcN6ndehAH379jXinJycsO4NeG3SiY+Pt3JXXHFFaQwn6jGTBABAQZMEAEBBkwQAQBE1a5KzZ8+2cnl5eVZuwYIFpTGcEpiYEJgAAAXwSURBVHfnnXf6PQRE2HXXXWfEDzzwQFj32bp1q5X7z3/+E9a9gHDFyn6PC8VMEgAABU0SAAAFTRIAAAVNEgAARdRs3PESK5t0vJ5wMmnSpKCv89q4lJ2dHZExIfKGDh1qxDVr1gz6Gq8DM9LT061c1apVjfiaa66xam688UYjXr16tVXz/fffG/HOnTuDjhGxxb3BTPPtt9+W8EjKBmaSAAAoaJIAAChokgAAKKJ6TbKsGjx4sBEPGDDAqqlYsWLQ+wwbNszKLVmyJPyBIep4HWZep04dK5eRkWHEzZs3D+v9jhw5YsQtWrSwatzrlogtDRo0CKluzZo1JTySsoGZJAAACpokAAAKmiQAAAqaJAAAioCI8Kj7CzBmzBgrN2HCBCOOi7P/LlJUVHTe14iITJw48QJHh5Jy0UUXWblPP/3UiDt37hz0PgUFBVbO/dkQEbn44ouLMbrQeX3Gxo8fXyLvheiwb98+K1elShUr5z7AorxiJgkAgIImCQCAgiYJAICCJgkAgIITd36yfPlyK+c4wfc0/fKXv7Ry7o0XXpsz/vWvfxnx7Nmzg74XosfZs2et3A8//FDs+6SkpERiOGGrVauWr++Pknf55ZcbsdeGnGXLlpXWcMocZpIAAChokgAAKGiSAAAoyu2a5J133mnEHTt2tGq8vtQdDq+ngCxYsCAi94Y/KlSoYOVSU1NL7P3cT+YYOHCgVZOenm7ETZs2LbHxoOyoVq2aEZfUwRSxipkkAAAKmiQAAAqaJAAACpokAACKcrtxZ/Xq1SV279GjRxsxm3Rij3szhIhIhw4dInLvWbNmWTn3kzl69epl1TRo0CAi74/YEsrn0utwDPwvZpIAAChokgAAKGiSAAAoAiIS/BTvcsDry9mjRo2ycqF8QXvdunVG3KlTp/AHhqj0wgsvWLnhw4cX+z5btmyxck8//bSVa9OmjRGPHDnSqklMTAz6frt37zbiLl26WDX79+8Peh9Ep4QEe5vJtm3bjPiqq66yarwOU1mzZk3kBlaGMZMEAEBBkwQAQEGTBABAQZMEAEDBxp3zqFevnpXbs2dPse8zZ84cK3fvvfeGMyT4pEaNGka8d+9eq6ZSpUpB7xMIBIz4zJkzVo3j2P+XDGVTTihat25txFu3bo3IfVHymjdvbuXat29vxF4bCx999NGg9/bauJOXl2fEx48ft2q+/fbboPd2bw774osvrJr8/Pyg9/ELM0kAABQ0SQAAFDRJAAAUJXLAeUpKihGnpaVZNe5/73bHpc1r/bF27dpWrqioqNj39lpjQtkSF2f+fTKU9Ucv7s9ChQoVwh6Tm3u9fPLkyVbNV199FbH3Q8lyry9u2rTJqklKSjLicH/XrFq1ysq573Xy5EmrJicnx4izsrKsmrZt2wat6dy5s5VzH3zhF2aSAAAoaJIAAChokgAAKGiSAAAoSmTjjvup6W+++aZVs3LlSiNevXp1SPd2fxk7UptinnrqKSsXziYdLwsWLIjIfYD/43WYwcSJE4147ty5pTUclICdO3casftACxGRu+66y4gjuUmwVq1aRly/fn2rxr0p87bbbrNq3AdWeH12CwoKwhliqWAmCQCAgiYJAICCJgkAgIImCQCAokSeAtK1a1cjfvLJJ62adu3ahXVv98knkdpc477vhdzbveD+i1/8Iqz7IHq4N4zdeeedVk2rVq2C3qdmzZpGPHjwYKvG68kK7ifJzJo1y6o5cOBA0PcHUDzMJAEAUNAkAQBQ0CQBAFCUyJqkm/upICIis2fPNuLu3buHdK/SXJM8cuSIlfvDH/4Q9F5r1641YvfTGQAAZQMzSQAAFDRJAAAUNEkAABQ0SQAAFKWycQcAgLKImSQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAAAKmiQAAAqaJAAACpokAACK/wHdkaanudTGmQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_HOSjTfRCoM"
      },
      "source": [
        "# creating a custom data set for loading image and radom number\n",
        "class CustomDataSet(Dataset):\n",
        "  def __init__(self, mnist_dataset):\n",
        "    self.mnist = mnist_dataset\n",
        "    self.numbers = torch.randint(10, size=(len(self),))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    #unpack img and label from mnist\n",
        "    img, label = self.mnist[index]\n",
        "    #get number from random number tensor\n",
        "    number = self.numbers[index]\n",
        "    #one hot encoding number\n",
        "    num_onehot = F.one_hot(number, num_classes = 10).squeeze().float()\n",
        "    #dictionary with input and target\n",
        "    return {'input': [img.to(device), num_onehot.to(device)], 'target':[label, label + number.item()]}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jafwxHfZRm_8"
      },
      "source": [
        "\n",
        "#instantiating dataset\n",
        "dataset = CustomDataSet(mnist_train_data)\n",
        "# NOTE :Basically this contains 60000 dictionary  of 'input':[(1x28X28),(1X10)] i.e image and one hot encoded random number\n",
        "                                                    # target:[int ,int] i.e label of image and label+random number\n",
        "\n",
        "#instantiating data loader with batch size 10\n",
        "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "#data_loader =dataset divded into batches of 10 input \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPS96TSEdt2m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oGY52XgRpS0",
        "outputId": "897c49f5-c7cf-4665-eebe-701b10d83a60"
      },
      "source": [
        "#get batch from data leader\n",
        "batch = next(iter(data_loader))\n",
        "inputs, targets = batch['input'], batch['target']\n",
        "print(\"Image batch {0}, One-hot encoded Number {1}\\n\\\n",
        "Handwritten digit label {2}, Sum of number and digit {3}\".format(inputs[0].shape, inputs[1].shape, targets[0].shape, targets[1].shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image batch torch.Size([10, 1, 28, 28]), One-hot encoded Number torch.Size([10, 10])\n",
            "Handwritten digit label torch.Size([10]), Sum of number and digit torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrpAJ06ddeWZ"
      },
      "source": [
        "# LETS CREATE TRAIN AND TEST LOADER\n",
        "#STEP 1: Download data \n",
        "#STEP 2: pass downloaded data through CustomDataSet\n",
        "#STEP 3: dataloader\n",
        "#TRAINLOADER \n",
        "train_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       #transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
        "                                       #transforms.Normalize((0.1307,), (0.3081,)) # The mean and std have to be sequences (e.g., tuples), therefore you should add a comma after the values. \n",
        "                                       # Note the difference between (0.1307) and (0.1307,)\n",
        "                                       ])\n",
        "\n",
        "# Test Phase transformations\n",
        "test_transforms = transforms.Compose([\n",
        "                                       transforms.ToTensor(),\n",
        "                                       #transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                       ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVNvGd2nh8CV"
      },
      "source": [
        "mnist_train_data = datasets.MNIST(\n",
        "    root='./data/',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transforms\n",
        ")\n",
        "train_dataset = CustomDataSet(mnist_train_data)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBrbLsqjdxtv"
      },
      "source": [
        "mnist_test_data = datasets.MNIST(\n",
        "    root='./data/',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=test_transforms\n",
        ")\n",
        "test_dataset = CustomDataSet(mnist_test_data)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ASRSsed65M",
        "outputId": "09d90e98-167d-4526-925c-5bb5928853d8"
      },
      "source": [
        "len(train_dataset),len(test_dataset),len(train_data_loader),len(test_data_loader)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000, 235, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDa8aklyeiZ7",
        "outputId": "d31dd61b-6644-406a-a4b6-f787963c3c45"
      },
      "source": [
        "batch = next(iter(test_data_loader))\n",
        "inputs, targets = batch['input'], batch['target']\n",
        "print(len(inputs[0]))\n",
        "print(\"Image batch {0}, One-hot encoded Number {1}\\n\\\n",
        "Handwritten digit label {2}, Sum of number and digit {3}\".format(inputs[0].shape, inputs[1].shape, targets[0].shape, targets[1].shape))\n",
        "print(targets[0][0])\n",
        "print(inputs[1][0])\n",
        "print(targets[1][0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "Image batch torch.Size([256, 1, 28, 28]), One-hot encoded Number torch.Size([256, 10])\n",
            "Handwritten digit label torch.Size([256]), Sum of number and digit torch.Size([256])\n",
            "tensor(2)\n",
            "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "tensor(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g__uNZkSARZ5"
      },
      "source": [
        "class Network1(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #IMAGE 1X1X28 => 1X1X10 \n",
        "      self.input_block = nn.Sequential(\n",
        "      #INPUT 28X28X1 >>CONV 3X3X1X16 >>26X26X16\n",
        "      nn.Conv2d(1, 16, 3, bias=False), \n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(16),\n",
        "      nn.Dropout2d(0.1),\n",
        "\n",
        "\n",
        "      #INPUT 26X26X16 >>CONV 3X3X16X32 >>24X24X32\n",
        "      nn.Conv2d(16, 32, 3, bias=False), \n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.Dropout2d(0.1),\n",
        "      )\n",
        "        # translation layer\n",
        "\n",
        "      self.trans1 = nn.Sequential(\n",
        "      #24X24x32 >>CONV 1X1X32X8 >>24X24X8\n",
        "      nn.Conv2d(32, 8, 1, bias=False), \n",
        "      nn.ReLU(),\n",
        "      #24X24x8 >>MAXPOOL (2,2) >>12X12X8\n",
        "      nn.MaxPool2d(2, 2),\n",
        "      )\n",
        "      self.conv_block = nn.Sequential(\n",
        "      #12X12x8 >>CONV 3X3X8X16 PAD=1 >>12X12X16\n",
        "      nn.Conv2d(8, 16, 3,padding=1, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(16),\n",
        "      nn.Dropout2d(0.1),\n",
        "\n",
        "      #12X12x16 >>CONV 3X3X16X32 >>10X10X32\n",
        "\n",
        "      nn.Conv2d(16, 32, 3, bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.Dropout2d(0.1),\n",
        "\n",
        "      )\n",
        "              # translation layer\n",
        "      self.trans2 = nn.Sequential(\n",
        "      #10X10x32 >>CONV 1X1X32X8 >>10X10X8\n",
        "      nn.Conv2d(32, 8, 1, bias=False), \n",
        "      nn.ReLU(),\n",
        "      #10X10x8 >>MAXPOOL (2,2) >>5X5X8\n",
        "      nn.MaxPool2d(2, 2),\n",
        "      )\n",
        "      self.conv_block2 = nn.Sequential(\n",
        "      #5X5X8 >>CONV 3X3X8X16 PAD=1 >>5X5X16\n",
        "      nn.Conv2d(8, 16, 3,padding=1,bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(16),\n",
        "      nn.Dropout2d(0.1),  \n",
        "      #5X5X16 >>CONV 3X3X8X32 PAD=0 >>3X3X32\n",
        "      nn.Conv2d(16, 32, 3,bias=False),\n",
        "      nn.ReLU(),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.Dropout2d(0.1),\n",
        "\n",
        "      )\n",
        "\n",
        "\n",
        "      self.avg_pool = nn.Sequential(\n",
        "      #3X3X32 >>CONV 1X1X32X10  >>3X3X10\n",
        "      nn.Conv2d(32, 10, 1, bias=False),\n",
        "      #3X3X10 >>AVG pool(3X3) >>1X1X10\n",
        "      nn.AvgPool2d(3))\n",
        "\n",
        "      ####################\n",
        "      #TEXT DATA \n",
        "      ###################  \n",
        "      #Random number block\n",
        "      #1X1X10  =>1X1X64\n",
        "      self.fc1 = nn.Linear(10, 64)\n",
        "      \n",
        "      #concated fc1 and flatten conv2\n",
        "      self.fc2 = nn.Linear(in_features=32*3*3 + 64, out_features=128)\n",
        "      self.fc3 = nn.Linear(in_features=128, out_features=64)\n",
        "      #self.fc4 = nn.Linear(in_features=256, out_features=64)\n",
        "\n",
        "      #output layers\n",
        "      self.out1 = nn.Linear(in_features=64, out_features=10)\n",
        "      self.out2 = nn.Linear(in_features=64, out_features=19)\n",
        "\n",
        "  def forward(self, t):\n",
        "      #unpacking image and numbers\n",
        "      images, numbers  = t\n",
        "      #IMAGE \n",
        "      x = self.input_block(images)\n",
        "      x = self.trans1(x)\n",
        "      x = self.conv_block(x)\n",
        "      x = self.trans2(x)\n",
        "      x = self.conv_block2(x)\n",
        "      x = x.reshape(-1, 32 * 3 * 3)\n",
        "      #x = self.avg_pool(x)\n",
        "      #x = x.view(-1, 10)\n",
        "      #NUMBERS\n",
        "\n",
        "      # sending random number to FC1\n",
        "      t1=x\n",
        "      t2 = self.fc1(numbers)\n",
        "      t2 = F.relu(t2)\n",
        "\n",
        "      # concatination of t1 and t2\n",
        "      t = torch.cat((t1, t2), 1)\n",
        "\n",
        "      # Propagating concatinated tensor to fully connected layers\n",
        "      t = self.fc2(t)\n",
        "      t = F.relu(t)\n",
        "\n",
        "      t = self.fc3(t)\n",
        "      t = F.relu(t)\n",
        "\n",
        "      # t = self.fc4(t)\n",
        "      # t = F.relu(t)\n",
        "\n",
        "      # Out1 Image Digit Prediction\n",
        "      o1 = self.out1(t)\n",
        "      o1 = F.softmax(o1, dim=1)\n",
        "\n",
        "      # Out2 Sum prediction\n",
        "      o2 = self.out2(t)\n",
        "      o2 = F.softmax(o2, dim=1)\n",
        "\n",
        "      return o1, o2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1M421RqyDCDt",
        "outputId": "422d67bf-01cb-4a63-b2a6-764d6cf395a5"
      },
      "source": [
        "model = Network1().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network1(\n",
            "  (input_block): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout2d(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (5): ReLU()\n",
            "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Dropout2d(p=0.1, inplace=False)\n",
            "  )\n",
            "  (trans1): Sequential(\n",
            "    (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout2d(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (5): ReLU()\n",
            "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Dropout2d(p=0.1, inplace=False)\n",
            "  )\n",
            "  (trans2): Sequential(\n",
            "    (0): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv_block2): Sequential(\n",
            "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Dropout2d(p=0.1, inplace=False)\n",
            "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
            "    (5): ReLU()\n",
            "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Dropout2d(p=0.1, inplace=False)\n",
            "  )\n",
            "  (avg_pool): Sequential(\n",
            "    (0): Conv2d(32, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
            "  )\n",
            "  (fc1): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=352, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (out1): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (out2): Linear(in_features=64, out_features=19, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-LaKrrKnMAq"
      },
      "source": [
        "#computing loss function for two softmax outputs\n",
        "def loss_function(outputs, t1, t2):\n",
        "  #unpacking outputs\n",
        "  o1, o2 = outputs\n",
        "  #computing cross entropy loss\n",
        "  l1=torch.nn.functional.cross_entropy(o1,t1)\n",
        "  l2=torch.nn.functional.cross_entropy(o2,t2)\n",
        "  #l1 = nn.CrossEntropyLoss()(o1, t1)\n",
        "  #l2 = nn.CrossEntropyLoss()(o2, t2)\n",
        "\n",
        "  #computing average loss\n",
        "  return l1+l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xxoHB7Cjdxc"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc_image = []\n",
        "train_acc_adder = []\n",
        "test_acc = []\n",
        "\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  #model.train()\n",
        "  #pbar = tqdm(train_loader)\n",
        "  correct = 0\n",
        "  digits_correct=0\n",
        "  sums_correct=0\n",
        "  processed = 0\n",
        "  #for batch_idx, batch in enumerate(pbar):\n",
        "  for batch in train_loader:\n",
        "    # get samples\n",
        "    inputs, targets = batch['input'], batch['target']\n",
        "    t1, t2 = targets[0].to(device), targets[1].to(device)\n",
        "    #data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # Init\n",
        "    optimizer.zero_grad()\n",
        "    # In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch accumulates the gradients on subsequent backward passes. \n",
        "    # Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the parameter update correctly.\n",
        "\n",
        "    # Predict\n",
        "    preds = model(inputs)\n",
        "    loss = loss_function(preds, t1, t2)\n",
        "\n",
        "\n",
        "    # Calculate loss\n",
        "    #loss = F.nll_loss(y_pred, target)\n",
        "    train_losses.append(loss)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update pbar-tqdm\n",
        "    digits_correct += preds[0].argmax(1).eq(t1).sum().item()\n",
        "    sums_correct += preds[1].argmax(1).eq(t2).sum().item()\n",
        "\n",
        "\n",
        "    processed += len(inputs[0])\n",
        "    #print(processed)\n",
        "    #pbar.set_description(desc= f'Loss={loss.item()} Batch_id={batch_idx}/{len(train_loader)} image_accuracy={100*digits_correct/processed:0.2f} adder_accuracy={100*sums_correct/processed:0.2f}')\n",
        "    train_acc_image.append(100*digits_correct/processed)\n",
        "    train_acc_adder.append(100*sums_correct/processed)\n",
        "    #tqdm._instances.clear()\n",
        "  # print(\"*****EPOCH SUMMARY IN TRAIN *****\")\n",
        "  # print(\"Epoch :\", epoch+1)\n",
        "  # print(\"Total images processed for the epoch\", processed)\n",
        "  # print(\"Images correct:\",digits_correct)\n",
        "  # print(\"Addition correct:\",sums_correct)\n",
        "  # print(\"Loss at the end of epoch\",train_losses[-1].item())\n",
        "  # print(\"Training Accuracy for image \",100*digits_correct/processed)\n",
        "  # print(\"Training Accuracy for Addition \",100*sums_correct/processed)\n",
        "  print(\"[TRAIN]: => loss {0}, total correct digits {1}, total correct sums {2} ,[image_accuracy]={3} , [adder_accuracy]={4} \".format(train_losses[-1].item(), digits_correct,sums_correct,\n",
        "                                                                                                                                   round((100*digits_correct/processed),2),round((100*sums_correct/processed),2)))\n",
        "  return train_losses[-1].item(),round((100*digits_correct/processed),2),round((100*sums_correct/processed),2)\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    digits_correct = 0\n",
        "    sums_correct = 0\n",
        "    processed = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            inputs, targets = batch['input'], batch['target']\n",
        "            t1, t2 = targets[0].to(device), targets[1].to(device)\n",
        "            preds = model(inputs)\n",
        "            loss = loss_function(preds, t1, t2)\n",
        "            test_losses.append(loss)\n",
        "            digits_correct += preds[0].argmax(1).eq(t1).sum().item()\n",
        "            sums_correct += preds[1].argmax(1).eq(t2).sum().item()\n",
        "            processed += len(inputs[0])\n",
        "    # print(\"*****EPOCH SUMMARY IN TEST *****\")\n",
        "    # print(\"Images correct:\",digits_correct)\n",
        "    # print(\"Addition correct:\",sums_correct)\n",
        "    # print(\"Loss at the end of epoch\",test_losses[-1].item())\n",
        "    # print(\"Total images processed for the epoch\",processed) \n",
        "    # print(\"Testing Accuracy for image \",100*digits_correct/processed)\n",
        "    # print(\"Testing  Accuracy for Addition \",100*sums_correct/processed) \n",
        "    print(\"[TEST] : => loss {0}, total correct digits {1}, total correct sums {2} ,[image_accuracy]={3} , [adder_accuracy]={4} \".format(test_losses[-1].item(), digits_correct,sums_correct,\n",
        "                                                                                                                           round((100*digits_correct/processed),2),round((100*sums_correct/processed),2)))\n",
        "    return test_losses[-1].item(),round((100*digits_correct/processed),2),round((100*sums_correct/processed),2)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtTqfV0gkaUl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMDWl_sujd_H",
        "outputId": "f8039fe6-f167-49d3-93a1-59fba965c543"
      },
      "source": [
        "model = Network1().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimiser = optim.Adam(model.parameters(), lr=0.01)\n",
        "EPOCHS = 5\n",
        "train_loss_per_epoch=[]\n",
        "train_image_acc_per_epoch=[]\n",
        "train_adder_accuracy_per_epoch=[]\n",
        "test_loss_per_epoch=[]\n",
        "test_image_acc_per_epoch=[]\n",
        "test_adder_accuracy_per_epoch=[]\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH:\", epoch)\n",
        "    train_epoch_loss,train_image_accuracy,train_adder_accuracy=train(model, device, train_data_loader, optimizer, epoch)\n",
        "    train_loss_per_epoch.append(train_epoch_loss)\n",
        "    train_image_acc_per_epoch.append(train_image_accuracy)\n",
        "    train_adder_accuracy_per_epoch.append(train_adder_accuracy)\n",
        "\n",
        "    test_epoch_loss,test_image_accuracy,test_adder_accuracy=test(model, device, test_data_loader)\n",
        "    test_loss_per_epoch.append(test_epoch_loss)\n",
        "    test_image_acc_per_epoch.append(test_image_accuracy)\n",
        "    test_adder_accuracy_per_epoch.append(test_adder_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH: 0\n",
            "[TRAIN]: => loss 5.244495391845703, total correct digits 7193, total correct sums 4597 ,[image_accuracy]=11.99 , [adder_accuracy]=7.66 \n",
            "[TEST] : => loss 5.239509582519531, total correct digits 1679, total correct sums 832 ,[image_accuracy]=16.79 , [adder_accuracy]=8.32 \n",
            "EPOCH: 1\n",
            "[TRAIN]: => loss 5.2393269538879395, total correct digits 11455, total correct sums 4956 ,[image_accuracy]=19.09 , [adder_accuracy]=8.26 \n",
            "[TEST] : => loss 5.242795944213867, total correct digits 2208, total correct sums 927 ,[image_accuracy]=22.08 , [adder_accuracy]=9.27 \n",
            "EPOCH: 2\n",
            "[TRAIN]: => loss 4.824916839599609, total correct digits 15437, total correct sums 5451 ,[image_accuracy]=25.73 , [adder_accuracy]=9.09 \n",
            "[TEST] : => loss 4.738735675811768, total correct digits 5340, total correct sums 915 ,[image_accuracy]=53.4 , [adder_accuracy]=9.15 \n",
            "EPOCH: 3\n",
            "[TRAIN]: => loss 4.5875749588012695, total correct digits 40876, total correct sums 6006 ,[image_accuracy]=68.13 , [adder_accuracy]=10.01 \n",
            "[TEST] : => loss 4.559754371643066, total correct digits 7341, total correct sums 964 ,[image_accuracy]=73.41 , [adder_accuracy]=9.64 \n",
            "EPOCH: 4\n",
            "[TRAIN]: => loss 4.6146979331970215, total correct digits 44636, total correct sums 6062 ,[image_accuracy]=74.39 , [adder_accuracy]=10.1 \n",
            "[TEST] : => loss 4.576569557189941, total correct digits 7622, total correct sums 948 ,[image_accuracy]=76.22 , [adder_accuracy]=9.48 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ce5qsNmobg_"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set()\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "fig, axs = plt.subplots(3,2,figsize=(15,10))\n",
        "axs[0, 0].plot(train_loss_per_epoch)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[0, 1].plot(train_loss_per_epoch)\n",
        "axs[0, 1].set_title(\"Test Loss\")\n",
        "axs[1, 0].plot(train_image_acc_per_epoch)\n",
        "axs[1, 0].set_title(\"Training Accuracy Image\")\n",
        "axs[1, 1].plot(test_image_acc_per_epoch)\n",
        "axs[1, 1].set_title(\"Test Accuracy Image\")\n",
        "axs[2, 0].plot(train_adder_accuracy_per_epoch)\n",
        "axs[2, 0].set_title(\"Training Accuracy Image\")\n",
        "axs[2, 1].plot(test_adder_accuracy_per_epoch)\n",
        "axs[2, 1].set_title(\"Test Accuracy Image\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ixC9AYacU1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3KOpPCIacpy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5mgy5W0acw1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pgyqugyaczw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPLRaj4Uac19"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwLkgfM5ac5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwLyNRHRGAsN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}