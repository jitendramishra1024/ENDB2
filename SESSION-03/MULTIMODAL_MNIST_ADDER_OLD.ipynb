{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MULTIMODAL_MNIST_ADDER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTtpSEK_yhvr"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpOoRsIKy2kq"
      },
      "source": [
        "class MNISTNET(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(MNISTNET, self).__init__()\n",
        "\n",
        "      self.input_block = nn.Sequential(\n",
        "      #INPUT 28X28X1 >>CONV 3X3X1X16 >>26X26X16\n",
        "          nn.Conv2d(1, 16, 3, bias=False), \n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.Dropout2d(0.1),\n",
        "\n",
        " \n",
        "      #INPUT 26X26X16 >>CONV 3X3X16X32 >>24X24X32\n",
        "          nn.Conv2d(16, 32, 3, bias=False), \n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Dropout2d(0.1),\n",
        "      )\n",
        "            # translation layer\n",
        "      \n",
        "      self.trans1 = nn.Sequential(\n",
        "          #24X24x32 >>CONV 1X1X32X8 >>24X24X8\n",
        "          nn.Conv2d(32, 8, 1, bias=False), \n",
        "          nn.ReLU(),\n",
        "          #24X24x8 >>MAXPOOL (2,2) >>12X12X8\n",
        "          nn.MaxPool2d(2, 2),\n",
        "      )\n",
        "      self.conv_block = nn.Sequential(\n",
        "          #12X12x8 >>CONV 3X3X8X16 PAD=1 >>12X12X16\n",
        "          nn.Conv2d(8, 16, 3,padding=1, bias=False),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(16),\n",
        "          nn.Dropout2d(0.1),\n",
        "\n",
        "          #12X12x16 >>CONV 3X3X16X32 >>10X10X32\n",
        "\n",
        "         nn.Conv2d(16, 32, 3, bias=False),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm2d(32),\n",
        "          nn.Dropout2d(0.1),\n",
        "\n",
        "      )\n",
        "                  # translation layer\n",
        "      self.trans2 = nn.Sequential(\n",
        "          #10X10x32 >>CONV 1X1X32X8 >>10X10X8\n",
        "          nn.Conv2d(32, 8, 1, bias=False), \n",
        "          nn.ReLU(),\n",
        "          #10X10x8 >>MAXPOOL (2,2) >>5X5X8\n",
        "          nn.MaxPool2d(2, 2),\n",
        "      )\n",
        "      self.conv_block2 = nn.Sequential(\n",
        "        #5X5X8 >>CONV 3X3X8X16 PAD=1 >>5X5X16\n",
        "         nn.Conv2d(8, 16, 3,padding=1,bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.Dropout2d(0.1),  \n",
        "        #5X5X16 >>CONV 3X3X8X32 PAD=0 >>3X3X32\n",
        "        nn.Conv2d(16, 32, 3,bias=False),\n",
        "        nn.ReLU(),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.Dropout2d(0.1),\n",
        "\n",
        "      )\n",
        "\n",
        "          \n",
        "      self.avg_pool = nn.Sequential(\n",
        "      #3X3X32 >>CONV 1X1X32X10  >>3X3X10\n",
        "      nn.Conv2d(32, 10, 1, bias=False),\n",
        "      #3X3X10 >>AVG pool(3X3) >>1X1X10\n",
        "      nn.AvgPool2d(3)\n",
        "      )\n",
        "\n",
        "    \"\"\"forward: performs a forward pass when model(x) is called\n",
        "    Params\n",
        "        x: the input data\n",
        "    Returns\n",
        "        y: the output of the model\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x = self.input_block(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv_block(x)\n",
        "        x = self.trans2(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 10)\n",
        "        #return F.log_softmax(x)\n",
        "        return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-8oggI6y7_e",
        "outputId": "81e58281-76c2-4c6e-f509-05449879759d"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = MNISTNET().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             144\n",
            "              ReLU-2           [-1, 16, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 16, 26, 26]              32\n",
            "         Dropout2d-4           [-1, 16, 26, 26]               0\n",
            "            Conv2d-5           [-1, 32, 24, 24]           4,608\n",
            "              ReLU-6           [-1, 32, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 32, 24, 24]              64\n",
            "         Dropout2d-8           [-1, 32, 24, 24]               0\n",
            "            Conv2d-9            [-1, 8, 24, 24]             256\n",
            "             ReLU-10            [-1, 8, 24, 24]               0\n",
            "        MaxPool2d-11            [-1, 8, 12, 12]               0\n",
            "           Conv2d-12           [-1, 16, 12, 12]           1,152\n",
            "             ReLU-13           [-1, 16, 12, 12]               0\n",
            "      BatchNorm2d-14           [-1, 16, 12, 12]              32\n",
            "        Dropout2d-15           [-1, 16, 12, 12]               0\n",
            "           Conv2d-16           [-1, 32, 10, 10]           4,608\n",
            "             ReLU-17           [-1, 32, 10, 10]               0\n",
            "      BatchNorm2d-18           [-1, 32, 10, 10]              64\n",
            "        Dropout2d-19           [-1, 32, 10, 10]               0\n",
            "           Conv2d-20            [-1, 8, 10, 10]             256\n",
            "             ReLU-21            [-1, 8, 10, 10]               0\n",
            "        MaxPool2d-22              [-1, 8, 5, 5]               0\n",
            "           Conv2d-23             [-1, 16, 5, 5]           1,152\n",
            "             ReLU-24             [-1, 16, 5, 5]               0\n",
            "      BatchNorm2d-25             [-1, 16, 5, 5]              32\n",
            "        Dropout2d-26             [-1, 16, 5, 5]               0\n",
            "           Conv2d-27             [-1, 32, 3, 3]           4,608\n",
            "             ReLU-28             [-1, 32, 3, 3]               0\n",
            "      BatchNorm2d-29             [-1, 32, 3, 3]              64\n",
            "        Dropout2d-30             [-1, 32, 3, 3]               0\n",
            "           Conv2d-31             [-1, 10, 3, 3]             320\n",
            "        AvgPool2d-32             [-1, 10, 1, 1]               0\n",
            "================================================================\n",
            "Total params: 17,392\n",
            "Trainable params: 17,392\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.18\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 1.24\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Cp8L_tTzANb"
      },
      "source": [
        "class ADDERNET(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ADDERNET, self).__init__()\n",
        "\n",
        "        self.num_classes = 10\n",
        "        self.dims = (1, 28, 28)\n",
        "\n",
        "        #INPUT 28X28X1 => OUT 1X1X10\n",
        "        self.mnist_base = MNISTNET()\n",
        "\n",
        "        # IN: 20 (10 mnist + 10 OHE rand num)\n",
        "        #IN 1X1x20  #OUT 1X1X60\n",
        "        self.prefinal_layer1 = nn.Sequential(\n",
        "            nn.Linear(in_features=20, out_features=60, bias=False),\n",
        "            nn.BatchNorm1d(60),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        #IN 1X1x60  #OUT 1X1X60\n",
        "        self.prefinal_layer2 = nn.Sequential(\n",
        "            nn.Linear(in_features=60, out_features=60, bias=False),\n",
        "            nn.BatchNorm1d(60),\n",
        "            nn.ReLU(),\n",
        "            \n",
        "        )\n",
        "\n",
        "        # IN: 60 ; OUT: 10\n",
        "        self.mnist_final_layer = nn.Sequential(\n",
        "          nn.Linear(in_features=60, out_features=10, bias=False),\n",
        "        )\n",
        "\n",
        "        # IN: 60 ; OUT 19\n",
        "        self.adder_final_layer = nn.Sequential(\n",
        "        nn.Linear(in_features=60, out_features=19, bias=False)\n",
        "        )\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, mnist_img, rand_num):\n",
        "        rand_num = F.one_hot(rand_num, num_classes=self.num_classes)\n",
        "        \n",
        "        # mnist embedding: 1x10\n",
        "        mnist_embed = self.mnist_base(mnist_img)\n",
        "\n",
        "        # concat the mnist embedding and the random number = 10+10=20 features\n",
        "        ccat = torch.cat([mnist_embed, rand_num], dim=-1)\n",
        "        #IN 1X1x20  #OUT 1X1X60 \n",
        "        pre_out = self.prefinal_layer1(ccat)\n",
        "        #IN 60  # OUT 60 \n",
        "        pre_out = self.prefinal_layer2(pre_out)\n",
        "        #IN 60 OUT 10 \n",
        "        mnist_out = self.mnist_final_layer(pre_out)\n",
        "        #IN 60 OUT 19\n",
        "        adder_out = self.adder_final_layer(pre_out)\n",
        "\n",
        "        return mnist_out, adder_out\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "I5jw2mcZGIu0",
        "outputId": "ff83d78d-1a8c-482d-f510-35f05b4c0322"
      },
      "source": [
        "#!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = ADDERNET().to(device)\n",
        "summary(model, (1, 28, 28,1))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6c0b0758b3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADDERNET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'rand_num'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1xq900QEI3F"
      },
      "source": [
        "\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwLyNRHRGAsN"
      },
      "source": [
        "\n",
        "        model = ADDERNET().to(device)\n",
        "        from tqdm import tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output_mnist,output_adder = model(data,)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "        tqdm._instances.clear()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        (mnist_img, rand_num), (mnist_y, adder_y) = batch\n",
        "\n",
        "        mnist_pred, adder_pred = self(mnist_img, rand_num)\n",
        "\n",
        "        # both mnist and adder use cross entropy loss\n",
        "        mnist_loss = self.loss(mnist_pred, mnist_y)\n",
        "        adder_loss = self.loss(adder_pred, adder_y)\n",
        "\n",
        "        # final loss is sum of the two loss\n",
        "        loss = mnist_loss + adder_loss\n",
        "\n",
        "        return loss\n",
        "      \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        (mnist_img, rand_num), (mnist_y, adder_y) = batch\n",
        "\n",
        "        mnist_pred, adder_pred = self(mnist_img, rand_num)\n",
        "\n",
        "        mnist_loss = self.loss(mnist_pred, mnist_y)\n",
        "        adder_loss = self.loss(adder_pred, adder_y)\n",
        "\n",
        "        loss = mnist_loss + adder_loss\n",
        "\n",
        "        mnist_pred = torch.argmax(F.log_softmax(mnist_pred, dim=1), dim=1)\n",
        "        adder_pred = torch.argmax(F.log_softmax(adder_pred, dim=1), dim=1)\n",
        "\n",
        "        mnist_acc = accuracy(mnist_pred, mnist_y)\n",
        "        adder_acc = accuracy(adder_pred, adder_y)\n",
        "\n",
        "        # Calling self.log will surface up scalars for you in TensorBoard\n",
        "        self.log('val_loss', loss, prog_bar=True)\n",
        "        self.log('mnist_acc', mnist_acc, prog_bar=True)\n",
        "        self.log('adder_acc', adder_acc, prog_bar=True)\n",
        "        self.log('total_acc', mnist_acc * adder_acc, prog_bar=True)\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}