{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_SENTIMENT_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrZyEw-umkaX"
      },
      "source": [
        "##OBJECTIVE \n",
        "##SENTIMENT CLASSIFICATION  FOR IMDB USING RNN\n",
        "## REF :https://github.com/bentrevett/pytorch-sentiment-analysis"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTZ3uWB-mzrl"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import random\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6z2IJjym_0K"
      },
      "source": [
        "def get_GPU_CPU(seed_val = 1):\n",
        "    print('The Seed is set to {}'.format(seed_val))\n",
        "    if torch.cuda.is_available():\n",
        "        print('Model will Run on CUDA.')\n",
        "        torch.cuda.manual_seed(seed_val)\n",
        "        !nvidia-smi\n",
        "        device = 'cuda'\n",
        "    else:\n",
        "        torch.manual_seed(seed_val)\n",
        "        print ('Running in CPU')\n",
        "        device = 'cpu'\n",
        "    cuda = torch.cuda.is_available()\n",
        "    return cuda,seed_val,device"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh5uEZ6KnQ40",
        "outputId": "9d648fcf-0a2e-4e4a-aa90-78210e472eb0"
      },
      "source": [
        "cuda,SEED,device = get_GPU_CPU(seed_val=1234)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Seed is set to 1234\n",
            "Model will Run on CUDA.\n",
            "Thu May 27 13:24:14 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    37W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L4L6p49nVaz"
      },
      "source": [
        "# make your experiment reproducible, similar to set random seed to all options where there needs a random seed\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDRxCpNIoIYz"
      },
      "source": [
        "#here spacy is a tokenizer \n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm')\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVJCbX6ccvrE"
      },
      "source": [
        "#WHY FLOAT  FOR LABEL:\n",
        "# when initializing the LABEL field, we set dtype=torch.float. This is because TorchText sets tensors to be LongTensors by default,\n",
        "# however our criterion expects both inputs to be FloatTensors. Setting the dtype to be torch.float, did this for us.\n",
        "# The alternative method of doing this would be to do the conversion inside the train function by passing batch.label.float() \n",
        "#instad of batch.label to the criterion."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v4icOPSo7-a"
      },
      "source": [
        "#DOWNLOAD DATA  AND SPLIT DATA INTO TRAIN AND TEST\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdVim25opCCO"
      },
      "source": [
        "#CHECK LENGTH OF TRAIN AND TEST DATA \n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcvcNs9RqxaF"
      },
      "source": [
        "#SEE A SAMPLE DATA \n",
        "print(vars(train_data.examples[0]))\n",
        "#This will return a dictionary with key as \"text \" and \"labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOYK1KfcpkY2"
      },
      "source": [
        "#DIVIDE TRAIN  DATA INTO TRAIN AND VAILID IN 80:20 RATIO\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMqJuSrkpztJ"
      },
      "source": [
        "# CHECK LENGTH OF TRAIN TEST AND VALID\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMrh3aKEqL6P"
      },
      "source": [
        "#Construct the Vocab object for this field from one or more datasets.\n",
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "\n",
        "#This is basically assigning a unique number to each word . one document may contain 10 lakh element but we should not \n",
        "#take 10 lakhs unique index instead take 25000 most repeated word and use them for assigning index to words .\n",
        "#so What happens if a word encounted which is not in most frequent 25000 in tht case that word is assigned with <unk> i,e unknown "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFNOLDxHvCI1"
      },
      "source": [
        "#Why do we only build the vocabulary on the training set? \n",
        "#When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lvK_nscqnwJ"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJXi_9GevgsY"
      },
      "source": [
        "#Why is the vocab size 25002 and not 25000?\n",
        "#One of the addition tokens is the <unk> token and the other is a <pad> token.\n",
        "\n",
        "#When we feed sentences into our model, we feed a batch of them at a time, i.e. more than one at a time, and all sentences in the batch need to be the same size.\n",
        "# Thus, to ensure each sentence in the batch is the same size, any shorter than the longest within the batch are padded.\n",
        "\n",
        "#sentenc1 : I live in bangalore and this is good\n",
        "#sentence2: I am   a  boy       <pad><pad><pad><pad>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_34OXourNpD"
      },
      "source": [
        "#MOST COMMON WORDS \n",
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUrV31srSSu"
      },
      "source": [
        "\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "#Create Iterator objects for multiple splits of a dataset.\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#POINT 01:creating the iterators. We iterate over these in the training/evaluation loop, and they return a batch of examples (indexed and converted into tensors) at each iteration.\n",
        "\n",
        "#POINT 02 :BucketIterator which is a special type of iterator that will return a batch of examples where each example is of a similar length, minimizing the amount of padding per example.\n",
        "\n",
        "#POINT 03 :We also want to place the tensors returned by the iterator on the GPU (if you're using one). PyTorch handles this using torch.device, we then pass this device to the iterator.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vWrpadprmNr"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #output = [sent len, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFsfEhZy-f6V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK7jdMUTyGWu"
      },
      "source": [
        "# In RNN we have 3 layers \n",
        "#IN RNN\n",
        "#----------\n",
        "#EMbedding layer :The embedding layer is used to transform our sparse one-hot vector (sparse as most of the elements are 0) into a dense embedding vector. It is a FC layer\n",
        "#RNN Layer :The RNN layer is our RNN which takes in our dense vector and the previous hidden state ht-1, which it uses to calculate the next hidden state, ht\n",
        "#OUTPUT Layer :Finally, the linear layer takes the final hidden state and feeds it through a fully connected layer, ht, transforming it to the correct output dimension.\n",
        "\n",
        "#IN Forwrd layer :\n",
        "#---------------\n",
        "#Each batch, text, is a tensor of size [sentence length, batch size]. That is a batch of sentences, each having each word converted into a one-hot vector.\n",
        "#i.e batch 1 :64 sentences , each sentence 772 length  and each word is onehot encoded (see below ). It is observed that each batch has different sentence length but with in the batch \n",
        "#same sentence length required \n",
        "\n",
        "#The input batch is then passed through the embedding layer to get embedded, which gives us a dense vector representation of our sentences.\n",
        "# embedded is a tensor of size [sentence length, batch size, embedding dim].\n",
        "\n",
        "#embedded is then fed into the RNN. In some frameworks you must feed the initial hidden state, ho, into the RNN, however in PyTorch,\n",
        "# if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "\n",
        "#OUTPUT OF RNN:\n",
        "#The RNN returns 2 tensors, output of size [sentence length, batch size, hidden dim] and hidden of size [1, batch size, hidden dim].\n",
        "#output is the concatenation of the hidden state from every time step, whereas hidden is simply the final hidden state\n",
        "\n",
        "#AASERT:\n",
        "#We verify this using the assert statement. Note the squeeze method, which is used to remove a dimension of size 1.#\n",
        "\n",
        "#Finally, we feed the last hidden state, hidden, through the linear layer, fc, to produce a prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzcChXXQ0l5-"
      },
      "source": [
        "print('Train')\n",
        "for batch in train_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    print(f'Text matrix size: {batch.text.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    print(f'Text matrix size: {batch.text.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_iterator:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    print(f'Text matrix size: {batch.text.size()}')\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFGnyUmm0mgC"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7MruILG7puJ"
      },
      "source": [
        "#MEANING :\n",
        "#The input dimension is the dimension of the one-hot vectors, which is equal to the vocabulary size.: HERE:len(TEXT.vocab)\n",
        "\n",
        "#The embedding dimension is the size of the dense word vectors. This is usually around 50-250 dimensions, but depends on the size of the vocabulary.: HERE :100\n",
        "\n",
        "#The hidden dimension is the size of the hidden states. This is usually around 100-500 dimensions, but also depends on factors such as on the vocabulary size, \n",
        "#the size of the dense vectors and the complexity of the task. : HERE :256\n",
        "\n",
        "#The output dimension is usually the number of classes, however in the case of only 2 classes the output value is between 0 and 1 and thus can be 1-dimensional,\n",
        "# i.e. a single scalar real number.: HERE :1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRt9Rsn0HGFc"
      },
      "source": [
        "# GET COUNT OF TRAINABLE PARAMETERS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cba2Z6oHKCs"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0HNRAVkHKvP"
      },
      "source": [
        "# DEFINE OPTIMIZER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLbWfaZSIF82"
      },
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4upv2GiIHRQ"
      },
      "source": [
        "#DEFINE LOSS\n",
        "#BCEWithLogitsLoss()\n",
        "#The loss function here is binary cross entropy with logits\n",
        "# We get output as real number . apply sigmoid to make it 0 to1 . We then use this this bound scalar to calculate the loss using binary cross entropy.\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6jVw55MJEkD"
      },
      "source": [
        "# PLACE MODEL AND LOSS TO GPU \n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km8ajjDRJe6u"
      },
      "source": [
        "##BINARY ACCURACY FUNCTION  PER BATCH \n",
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVutvX5tXJXL"
      },
      "source": [
        "#TRAINING\n",
        "\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJRgO0DhXPwT"
      },
      "source": [
        "#MODEL.TRAIN:\n",
        "#model.train() is used to put the model in \"training mode\", which turns on dropout and batch normalization. \n",
        "#Although we aren't using them in this model, it's good practice to include it.\n",
        "\n",
        "#OPTIMIZER.ZEROGRAD :\n",
        "#For each batch, we first zero the gradients. Each parameter in a model has a grad attribute which stores the gradient calculated by the criterion.\n",
        "#PyTorch does not automatically remove (or \"zero\") the gradients calculated from the last gradient calculation, so they must be manually zeroed.\n",
        "\n",
        "#CALL TO FORWARD:\n",
        "#We then feed the batch of sentences, batch.text, into the model. Note, you do not need to do model.forward(batch.text),\n",
        "#simply calling the model works. The squeeze is needed as the predictions are initially size [batch size, 1],\n",
        "# and we need to remove the dimension of size 1 as PyTorch expects the predictions input to our criterion function to be of size [batch size].\n",
        "\n",
        "#CALCULATE LOSS AND ACCURACY :\n",
        "#The loss and accuracy are then calculated using our predictions and the labels, batch.label, with the loss being averaged over all examples in the batch.\n",
        "\n",
        "#CALCULATE AND UPDATE GRADIENT :\n",
        "#We calculate the gradient of each parameter with loss.backward(), and then update the parameters using the gradients and optimizer algorithm with optimizer.step().\n",
        "\n",
        "#ACCUMULATE LOSS AND ACCURACY FOR ENTIRE SINGLE EPOCH \n",
        "#The loss and accuracy is accumulated across the epoch, the .item() method is used to extract a scalar from a tensor which only contains a single value.\n",
        "\n",
        "#RETURN AVARAGE ACCURACY AND LOSS\n",
        "#Finally, we return the loss and accuracy, averaged across the epoch. The len of an iterator is the number of batches in the iterator."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXpZnh-dLpX"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Y5aFxsdMOV"
      },
      "source": [
        "#evaluate is similar to train, with a few modifications as you don't want to update the parameters when evaluating.\n",
        "#MODEL.EVAL()\n",
        "#model.eval() puts the model in \"evaluation mode\", this turns off dropout and batch normalization. Again, we are not using them in this model, but it is good practice to include them.\n",
        "#TORCH.NO_GRAD()\n",
        "#No gradients are calculated on PyTorch operations inside the with no_grad() block.\n",
        "#This causes less memory to be used and speeds up computation.\n",
        "#REMOVE optimizer.zero_grad(), loss.backward() and optimizer.step()\n",
        "#The rest of the function is the same as train, with the removal of optimizer.zero_grad(), loss.backward() and optimizer.step(), as we do not update the model's parameters when evaluating."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "081fQb-GeVaQ"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZS-pEpJewDj"
      },
      "source": [
        "#We then train the model through multiple epochs, an epoch being a complete pass through all examples in the training and validation sets."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC73yPjLeWRb"
      },
      "source": [
        "N_EPOCHS = 30\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "#ACCUMULATE TRAIN TEST LOSS ACC\n",
        "train_loss_epoch=[]\n",
        "valid_loss_epoch=[]\n",
        "train_acc_epoch=[]\n",
        "valid_acc_epoch=[]\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    train_loss_epoch.append(round(train_loss,3))\n",
        "    valid_loss_epoch.append(round(valid_loss,3))\n",
        "    train_acc_epoch.append(round((train_acc*100),2))\n",
        "    valid_acc_epoch.append(round((valid_acc*100),2))\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cLNKOoE0x6t"
      },
      "source": [
        "# PLOT A GRAPH BETWEEN TRAIN TEST LOSS ACC FOR N EPOCHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ojk04z-g4-nA"
      },
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set()\n",
        "plt.style.use(\"dark_background\")\n",
        "\n",
        "fig, axs = plt.subplots(2,2,figsize=(15,10))\n",
        "axs[0, 0].plot(train_loss_epoch)\n",
        "axs[0, 0].set_title(\"Training Loss\")\n",
        "axs[0, 1].plot(valid_loss_epoch)\n",
        "axs[0, 1].set_title(\"Valid Loss\")\n",
        "axs[1, 0].plot(train_acc_epoch)\n",
        "axs[1, 0].set_title(\"Training Accuracy \")\n",
        "axs[1, 1].plot(valid_acc_epoch)\n",
        "axs[1, 1].set_title(\"Validation Accuracy\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEz_3NCU5wEf"
      },
      "source": [
        "# CALCULATE TEST SET LOSS AND ACCURACY "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxbFrYMdenUM"
      },
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}